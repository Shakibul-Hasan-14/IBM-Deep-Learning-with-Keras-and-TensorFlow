{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Custom Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Setup the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Custom Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.435736656188965\n",
      "Epoch 1 Step 200: Loss = 0.35337692499160767\n",
      "Epoch 1 Step 400: Loss = 0.199442058801651\n",
      "Epoch 1 Step 600: Loss = 0.1928015649318695\n",
      "Epoch 1 Step 800: Loss = 0.16863012313842773\n",
      "Epoch 1 Step 1000: Loss = 0.45767319202423096\n",
      "Epoch 1 Step 1200: Loss = 0.16339987516403198\n",
      "Epoch 1 Step 1400: Loss = 0.2781286835670471\n",
      "Epoch 1 Step 1600: Loss = 0.2289547473192215\n",
      "Epoch 1 Step 1800: Loss = 0.1782616823911667\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.07691352069377899\n",
      "Epoch 2 Step 200: Loss = 0.15640394389629364\n",
      "Epoch 2 Step 400: Loss = 0.10423268377780914\n",
      "Epoch 2 Step 600: Loss = 0.05262263864278793\n",
      "Epoch 2 Step 800: Loss = 0.08984110504388809\n",
      "Epoch 2 Step 1000: Loss = 0.2653706967830658\n",
      "Epoch 2 Step 1200: Loss = 0.07123812288045883\n",
      "Epoch 2 Step 1400: Loss = 0.19031307101249695\n",
      "Epoch 2 Step 1600: Loss = 0.15378209948539734\n",
      "Epoch 2 Step 1800: Loss = 0.09773679822683334\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Adding Accuracy Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Loss Function, Optimizer, and Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Custom Training Loop with Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.302577495574951 Accuracy = 0.15625\n",
      "Epoch 1 Step 200: Loss = 0.34634149074554443 Accuracy = 0.8339552283287048\n",
      "Epoch 1 Step 400: Loss = 0.19706083834171295 Accuracy = 0.8673628568649292\n",
      "Epoch 1 Step 600: Loss = 0.19793710112571716 Accuracy = 0.8826954960823059\n",
      "Epoch 1 Step 800: Loss = 0.16251564025878906 Accuracy = 0.8955602645874023\n",
      "Epoch 1 Step 1000: Loss = 0.3873234987258911 Accuracy = 0.9025974273681641\n",
      "Epoch 1 Step 1200: Loss = 0.19210542738437653 Accuracy = 0.9094244241714478\n",
      "Epoch 1 Step 1400: Loss = 0.2507779598236084 Accuracy = 0.9146145582199097\n",
      "Epoch 1 Step 1600: Loss = 0.2379835844039917 Accuracy = 0.9178052544593811\n",
      "Epoch 1 Step 1800: Loss = 0.17339758574962616 Accuracy = 0.921762228012085\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08006104826927185 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.17121076583862305 Accuracy = 0.9603545069694519\n",
      "Epoch 2 Step 400: Loss = 0.13195188343524933 Accuracy = 0.9571384191513062\n",
      "Epoch 2 Step 600: Loss = 0.030156638473272324 Accuracy = 0.9594426155090332\n",
      "Epoch 2 Step 800: Loss = 0.08376371115446091 Accuracy = 0.9610642790794373\n",
      "Epoch 2 Step 1000: Loss = 0.24990993738174438 Accuracy = 0.9616633653640747\n",
      "Epoch 2 Step 1200: Loss = 0.13988111913204193 Accuracy = 0.9625572562217712\n",
      "Epoch 2 Step 1400: Loss = 0.1758555769920349 Accuracy = 0.9635528326034546\n",
      "Epoch 2 Step 1600: Loss = 0.1511305570602417 Accuracy = 0.9634798765182495\n",
      "Epoch 2 Step 1800: Loss = 0.1073618233203888 Accuracy = 0.9639610052108765\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.033677902072668076 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.12041529268026352 Accuracy = 0.9765236377716064\n",
      "Epoch 3 Step 400: Loss = 0.13961324095726013 Accuracy = 0.9735037684440613\n",
      "Epoch 3 Step 600: Loss = 0.02203831449151039 Accuracy = 0.9745736122131348\n",
      "Epoch 3 Step 800: Loss = 0.052607499063014984 Accuracy = 0.9747191071510315\n",
      "Epoch 3 Step 1000: Loss = 0.17381882667541504 Accuracy = 0.9748376607894897\n",
      "Epoch 3 Step 1200: Loss = 0.08095309138298035 Accuracy = 0.975255012512207\n",
      "Epoch 3 Step 1400: Loss = 0.08754844963550568 Accuracy = 0.9756200909614563\n",
      "Epoch 3 Step 1600: Loss = 0.09636649489402771 Accuracy = 0.97523033618927\n",
      "Epoch 3 Step 1800: Loss = 0.04721444100141525 Accuracy = 0.9756385087966919\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.021123580634593964 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.09949605911970139 Accuracy = 0.9828979969024658\n",
      "Epoch 4 Step 400: Loss = 0.11253760010004044 Accuracy = 0.9816864132881165\n",
      "Epoch 4 Step 600: Loss = 0.023943018168210983 Accuracy = 0.982633113861084\n",
      "Epoch 4 Step 800: Loss = 0.033624179661273956 Accuracy = 0.9824048280715942\n",
      "Epoch 4 Step 1000: Loss = 0.12417003512382507 Accuracy = 0.9825174808502197\n",
      "Epoch 4 Step 1200: Loss = 0.04828153923153877 Accuracy = 0.9827747941017151\n",
      "Epoch 4 Step 1400: Loss = 0.041873108595609665 Accuracy = 0.9829809069633484\n",
      "Epoch 4 Step 1600: Loss = 0.05070126801729202 Accuracy = 0.9827061295509338\n",
      "Epoch 4 Step 1800: Loss = 0.03582378849387169 Accuracy = 0.983047604560852\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.026424840092658997 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.06221071258187294 Accuracy = 0.9866293668746948\n",
      "Epoch 5 Step 400: Loss = 0.10233323276042938 Accuracy = 0.9867518544197083\n",
      "Epoch 5 Step 600: Loss = 0.018435221165418625 Accuracy = 0.9874687790870667\n",
      "Epoch 5 Step 800: Loss = 0.011758930049836636 Accuracy = 0.9872034788131714\n",
      "Epoch 5 Step 1000: Loss = 0.06385760009288788 Accuracy = 0.9876373410224915\n",
      "Epoch 5 Step 1200: Loss = 0.031030718237161636 Accuracy = 0.9877966046333313\n",
      "Epoch 5 Step 1400: Loss = 0.012305364944040775 Accuracy = 0.9878658056259155\n",
      "Epoch 5 Step 1600: Loss = 0.021025894209742546 Accuracy = 0.9876834750175476\n",
      "Epoch 5 Step 1800: Loss = 0.02005445584654808 Accuracy = 0.9879233837127686\n"
     ]
    }
   ],
   "source": [
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Custom Callback for Advanced Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Loss Function, Optimizer, and Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Custom Training Loop with Custom Callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.2991480827331543 Accuracy = 0.09375\n",
      "Epoch 1 Step 200: Loss = 0.3921699821949005 Accuracy = 0.8369091749191284\n",
      "Epoch 1 Step 400: Loss = 0.19065356254577637 Accuracy = 0.8676745891571045\n",
      "Epoch 1 Step 600: Loss = 0.21626168489456177 Accuracy = 0.8820715546607971\n",
      "Epoch 1 Step 800: Loss = 0.19728605449199677 Accuracy = 0.8948969841003418\n",
      "Epoch 1 Step 1000: Loss = 0.3908486068248749 Accuracy = 0.9024413228034973\n",
      "Epoch 1 Step 1200: Loss = 0.158797025680542 Accuracy = 0.9082275032997131\n",
      "Epoch 1 Step 1400: Loss = 0.23864497244358063 Accuracy = 0.9131200909614563\n",
      "Epoch 1 Step 1600: Loss = 0.23799625039100647 Accuracy = 0.9163608551025391\n",
      "Epoch 1 Step 1800: Loss = 0.21275144815444946 Accuracy = 0.9203914403915405\n",
      "End of epoch 1, loss: 0.04152049496769905, accuracy: 0.9225500226020813\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.07057152688503265 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.18653284013271332 Accuracy = 0.9600435495376587\n",
      "Epoch 2 Step 400: Loss = 0.12794063985347748 Accuracy = 0.9569045901298523\n",
      "Epoch 2 Step 600: Loss = 0.05065469071269035 Accuracy = 0.9586106538772583\n",
      "Epoch 2 Step 800: Loss = 0.05915480852127075 Accuracy = 0.9601279497146606\n",
      "Epoch 2 Step 1000: Loss = 0.3098539710044861 Accuracy = 0.9606330990791321\n",
      "Epoch 2 Step 1200: Loss = 0.09243626892566681 Accuracy = 0.9611261487007141\n",
      "Epoch 2 Step 1400: Loss = 0.13070783019065857 Accuracy = 0.9619691371917725\n",
      "Epoch 2 Step 1600: Loss = 0.15195263922214508 Accuracy = 0.9619573950767517\n",
      "Epoch 2 Step 1800: Loss = 0.09741564840078354 Accuracy = 0.9627810716629028\n",
      "End of epoch 2, loss: 0.033083729445934296, accuracy: 0.9635499715805054\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Add Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Define Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 645us/step - loss: 0.6969 - accuracy: 0.4940\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 646us/step - loss: 0.6901 - accuracy: 0.5400\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 645us/step - loss: 0.6877 - accuracy: 0.5430\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 645us/step - loss: 0.6866 - accuracy: 0.5400\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 645us/step - loss: 0.6840 - accuracy: 0.5620\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.6837 - accuracy: 0.5690\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 517us/step - loss: 0.6809 - accuracy: 0.5810\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 645us/step - loss: 0.6790 - accuracy: 0.5750\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.6798 - accuracy: 0.5750\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 677us/step - loss: 0.6768 - accuracy: 0.5820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e6ef0a5640>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5050\n",
      "Test loss: 0.6914986371994019\n",
      "Test accuracy: 0.5049999952316284\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.03961790353059769\n",
      "Epoch 2: Loss = 0.04869624972343445\n",
      "Epoch 3: Loss = 0.05585509166121483\n",
      "Epoch 4: Loss = 0.04110407829284668\n",
      "Epoch 5: Loss = 0.028229668736457825\n"
     ]
    }
   ],
   "source": [
    "# Setup the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "# Implement the Custom Training Loop\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Adding Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.03587125241756439 Accuracy = 0.9247000217437744\n",
      "Epoch 2: Loss = 0.03263261169195175 Accuracy = 0.9649333357810974\n",
      "Epoch 3: Loss = 0.040505990386009216 Accuracy = 0.9765166640281677\n",
      "Epoch 4: Loss = 0.02516930177807808 Accuracy = 0.9824833273887634\n",
      "Epoch 5: Loss = 0.01805526204407215 Accuracy = 0.9873499870300293\n"
     ]
    }
   ],
   "source": [
    "# Setup the Environment\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Implement the Custom Training Loop with Accuracy Tracking\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "    accuracy_metric.reset_state() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Custom Callback for Advanced Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.05514167994260788, accuracy: 0.9241833090782166\n",
      "End of epoch 2, loss: 0.07070286571979523, accuracy: 0.9649999737739563\n",
      "End of epoch 3, loss: 0.0417470708489418, accuracy: 0.9762499928474426\n",
      "End of epoch 4, loss: 0.028979355469346046, accuracy: 0.9832333326339722\n",
      "End of epoch 5, loss: 0.013711516745388508, accuracy: 0.9883166551589966\n"
     ]
    }
   ],
   "source": [
    "# Setup the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Define the Model\n",
    "model = Sequential([ \n",
    "    tf.keras.Input(shape=(28, 28)),  # Updated Input layer syntax\n",
    "    Flatten(), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Implement the Custom Callback\n",
    "class CustomCallback(Callback): \n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "# Implement the Custom Training Loop with Custom Callback\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()  # Updated method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 01s]\n",
      "val_accuracy: 0.9300000071525574\n",
      "\n",
      "Best val_accuracy So Far: 0.9350000023841858\n",
      "Total elapsed time: 00h 00m 11s\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'tuner_results/hyperparam_tuning/trial_00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 55\u001b[0m\n\u001b[0;32m     48\u001b[0m         results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m\"\u001b[39m: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_hps\u001b[38;5;241m.\u001b[39mvalues,  \u001b[38;5;66;03m# Hyperparameters tuned in this trial\u001b[39;00m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Add any score or metrics if available\u001b[39;00m\n\u001b[0;32m     52\u001b[0m         }\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# Save the results as JSON\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtuner_results/hyperparam_tuning/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrial_0\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     56\u001b[0m             json\u001b[38;5;241m.\u001b[39mdump(results, f)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cvpr\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tuner_results/hyperparam_tuning/trial_00'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuner_results/hyperparam_tuning/', f\"trial_0{i}\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
